{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "Reinforcement Learning Course, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go through the code first to see what is going on. The first section reads the data and prepares the training, test and validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T06:28:24.224338Z",
     "start_time": "2018-10-26T06:28:23.947014Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load file and split input from target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset from:\n",
    "https://archive.ics.uci.edu/ml/datasets/Airfoil+Self-Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T06:28:25.231851Z",
     "start_time": "2018-10-26T06:28:25.197371Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"airfoil_self_noise.dat\",\"r\") as file:\n",
    "    csvreader = csv.reader(file, delimiter='\\t')\n",
    "    table = np.asarray([row for row in csvreader], dtype=np.float)\n",
    "np.set_printoptions(suppress=True) # do not show scientific notation\n",
    "print(table.shape)\n",
    "print(table[:3,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalize data and keep normalizing factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T06:28:26.292781Z",
     "start_time": "2018-10-26T06:28:26.287418Z"
    }
   },
   "outputs": [],
   "source": [
    "xs = table[:,0:-1]\n",
    "x_mean, x_std = np.mean(xs), np.std(xs)\n",
    "xs = (xs - x_mean)/x_std\n",
    "ys = table[:,[-1]]\n",
    "y_mean, y_std = np.mean(ys), np.std(ys)\n",
    "ys = (ys - y_mean)/y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T06:28:26.723166Z",
     "start_time": "2018-10-26T06:28:26.718178Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(xs.shape,ys.shape)\n",
    "print(ys[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function splits data into random train, val, test  (attention, we used a fixed seed here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-25T22:34:14.394179",
     "start_time": "2018-10-25T22:34:14.386805"
    }
   },
   "outputs": [],
   "source": [
    "def splitDataSetShuffle(inputs, outputs,percent_val_test=10, seed=1):\n",
    "    assert len(inputs) == len(outputs)\n",
    "    size = len(inputs)\n",
    "    np.random.seed(seed)\n",
    "    shuffle = np.random.permutation(size)\n",
    "    inps = np.asarray(inputs)[shuffle]\n",
    "    outs = np.asarray(outputs)[shuffle]\n",
    "    ts = size * (100-2*percent_val_test) // 100\n",
    "    vs = size * percent_val_test // 100\n",
    "    train_set = (inps[:ts], outs[:ts])\n",
    "    valid_set = (inps[ts:ts + vs], outs[ts:ts + vs])\n",
    "    test_set = (inps[ts + vs:], outs[ts + vs:])\n",
    "    return train_set, valid_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-25T22:34:14.547451",
     "start_time": "2018-10-25T22:34:14.543323"
    }
   },
   "outputs": [],
   "source": [
    "train_set, valid_set, test_set = splitDataSetShuffle(xs,ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-25T22:34:14.775684",
     "start_time": "2018-10-25T22:34:14.769920"
    }
   },
   "outputs": [],
   "source": [
    "print(train_set[0].shape, valid_set[0].shape, test_set[0].shape)\n",
    "print(train_set[1].shape, valid_set[1].shape, test_set[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 layer neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our simple Neural network implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-25T23:49:24.884870",
     "start_time": "2018-10-25T23:49:24.848543"
    }
   },
   "outputs": [],
   "source": [
    "class simpleNN(object):\n",
    "    def __init__(self, input_dim, output_dim, num_hidden_units, seed=2):        \n",
    "        # initialize weight matrices and bias vectors\n",
    "        np.random.seed(seed)\n",
    "        weight_variance =  2.0/(num_hidden_units + input_dim) # Xavier initialization\n",
    "        self.W1 = np.random.randn(input_dim, num_hidden_units) * weight_variance\n",
    "        self.b1 = np.zeros(num_hidden_units)\n",
    "        weight_variance =  2.0/(output_dim + num_hidden_units)\n",
    "        self.W2 = np.random.randn(num_hidden_units, output_dim) * weight_variance\n",
    "        self.b2 = np.zeros(output_dim)        \n",
    "        self.layer1 = None\n",
    "        \n",
    "    def evaluate(self, x):                    \n",
    "        self.layer1 = np.tanh(np.dot(x, self.W1) + self.b1)\n",
    "        return np.dot(self.layer1, self.W2) + self.b2        \n",
    "    \n",
    "    def train_with_square_loss(self, X, y, epsilon=0.001):\n",
    "        # implement this function\n",
    "        # X is the matrix of inputs of shape (num_examples, dimension)\n",
    "        # y is the vector of target outputs (num_examples, 1)\n",
    "        # loss function: L = Sum_i(f(x_i) - y)_i^2        \n",
    "        \n",
    "        # it can be useful to use the evaluate function in here\n",
    "        \n",
    "        # return prediction error during training        \n",
    "        return pred_error\n",
    "                \n",
    "    def error(self, X, y):\n",
    "        pred = self.evaluate(X)\n",
    "        return np.mean((pred - y)**2)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create an instance with 20 hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = simpleNN(train_set[0].shape[1], 1, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initial error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T00:02:22.938484",
     "start_time": "2018-10-26T00:02:22.932180"
    }
   },
   "outputs": [],
   "source": [
    "net.error(valid_set[0],valid_set[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform training using gradient decent (using all of the training data at once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T00:02:52.568735",
     "start_time": "2018-10-26T00:02:52.564574"
    }
   },
   "outputs": [],
   "source": [
    "train_errors = []\n",
    "val_errors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T00:04:53.268702",
     "start_time": "2018-10-26T00:04:34.273278"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(10000):    \n",
    "    train_errors.append(net.train_with_square_loss(train_set[0],train_set[1], 0.00005))    \n",
    "    # add here also the validation errors\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T00:04:53.564495",
     "start_time": "2018-10-26T00:04:53.269837"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(train_errors, color='g', linewidth=2)\n",
    "# plt.plot(val_errors, color='b', linestyle='--', linewidth=2)\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final performance on all the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T00:04:56.816477",
     "start_time": "2018-10-26T00:04:56.806782"
    }
   },
   "outputs": [],
   "source": [
    "print(net.error(*train_set),net.error(*valid_set),net.error(*test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform stochastic gradient descent (SGD) (with minibatches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T07:03:40.413613Z",
     "start_time": "2018-10-26T07:03:40.408265Z"
    }
   },
   "source": [
    "Create new instance of the network and perform training with mini-batches of size 16 with learning rate around 0.0002. You will need more update steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation the validation error every 100 batches and make the same calculation as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T07:19:27.414365Z",
     "start_time": "2018-10-26T07:19:27.403732Z"
    }
   },
   "source": [
    "The training curve will be very noise. Try to plot a smoothed version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use a larger network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T07:21:41.142624Z",
     "start_time": "2018-10-26T07:21:41.135433Z"
    }
   },
   "source": [
    "Do the same thing with 150 hidden units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T07:22:35.073375Z",
     "start_time": "2018-10-26T07:22:35.069015Z"
    }
   },
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "If you managed to incorporate the regularization, you can compare weights with and without it. You can use these matrix visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T07:19:53.668400Z",
     "start_time": "2018-10-26T07:19:53.656344Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(net.W1, interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T00:00:05.370424",
     "start_time": "2018-10-26T00:00:05.117144"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(net.W2.T,interpolation=\"nearest\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TD3_Ant.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXu1r8qvSzWf",
        "colab_type": "text"
      },
      "source": [
        "# Twin-Delayed DDPG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRzQUhuUTc0J",
        "colab_type": "text"
      },
      "source": [
        "## Installing the packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAHMB0Ze8fU0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "966b8082-7b8c-4a2f-d738-b4c544bb82fb"
      },
      "source": [
        "!pip install pybullet"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for pybullet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/c3/92/94/4f9c3e1b1769fc834a102b7ed04cc07ebad8663bfe11e2d830\n",
            "Successfully built pybullet\n",
            "Installing collected packages: pybullet\n",
            "Successfully installed pybullet-2.5.1\n",
            "Requirement already satisfied: pybullet in /usr/local/lib/python3.6/dist-packages (2.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xjm2onHdT-Av",
        "colab_type": "text"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ikr2p0Js8iB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pybullet_envs\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable\n",
        "from collections import deque"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2nGdtlKVydr",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: We initialize the Experience Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5rW0IDB8nTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "\n",
        "  def __init__(self, max_size=1e6):\n",
        "    self.storage = []\n",
        "    self.max_size = max_size\n",
        "    self.ptr = 0\n",
        "\n",
        "  def add(self, transition):\n",
        "    if len(self.storage) == self.max_size:\n",
        "      self.storage[int(self.ptr)] = transition\n",
        "      self.ptr = (self.ptr + 1) % self.max_size\n",
        "    else:\n",
        "      self.storage.append(transition)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
        "    for i in ind: \n",
        "      state, next_state, action, reward, done = self.storage[i]\n",
        "      batch_states.append(np.array(state, copy=False))\n",
        "      batch_next_states.append(np.array(next_state, copy=False))\n",
        "      batch_actions.append(np.array(action, copy=False))\n",
        "      batch_rewards.append(np.array(reward, copy=False))\n",
        "      batch_dones.append(np.array(done, copy=False))\n",
        "    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb7TTaHxWbQD",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: We build one neural network for the Actor model and one neural network for the Actor target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CeRW4D79HL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Actor(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRDDce8FXef7",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: We build two neural networks for the two Critic models and two neural networks for the two Critic targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCee7gwR9Jrs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Critic(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    # Defining the second Critic neural network\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    # Forward-Propagation on the first Critic Neural Network\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Forward-Propagation on the second Critic Neural Network\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzIDuONodenW",
        "colab_type": "text"
      },
      "source": [
        "## Steps 4 to 15: Training Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zzd0H1xukdKe",
        "colab": {}
      },
      "source": [
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "\n",
        "class TD3(object):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \n",
        "    for it in range(iterations):\n",
        "      \n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state)\n",
        "      \n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "      \n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "      \n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "      \n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "      \n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "      \n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "      \n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      \n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "  \n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  \n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka-ZRtQvjBex",
        "colab_type": "text"
      },
      "source": [
        "## We make a function that evaluates the policy by calculating its average reward over 10 episodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qabqiYdp9wDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGuKmH_ijf7U",
        "colab_type": "text"
      },
      "source": [
        "## We set the parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFj6wbAo97lk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env_name = \"AntBulletEnv-v0\" # Name of a environment (set it to any Continous environment you want)\n",
        "seed = 0 # Random seed number\n",
        "start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
        "eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)\n",
        "max_timesteps = 5e5 # Total number of iterations/timesteps\n",
        "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
        "expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n",
        "batch_size = 100 # Size of the batch\n",
        "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
        "tau = 0.005 # Target network update rate\n",
        "policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
        "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
        "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hjwf2HCol3XP",
        "colab_type": "text"
      },
      "source": [
        "## We create a file name for the two saved models: the Actor and Critic models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fyH8N5z-o3o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "42b02172-b01a-41e6-8702-8b0f605719c8"
      },
      "source": [
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_AntBulletEnv-v0_0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kop-C96Aml8O",
        "colab_type": "text"
      },
      "source": [
        "## We create a folder inside which will be saved the trained models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Src07lvY-zXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(\"./results\"):\n",
        "  os.makedirs(\"./results\")\n",
        "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
        "  os.makedirs(\"./pytorch_models\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEAzOd47mv1Z",
        "colab_type": "text"
      },
      "source": [
        "## We create the PyBullet environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyQXJUIs-6BV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "ed1b3525-5220-4816-b24c-afc07b4be01d"
      },
      "source": [
        "env = gym.make(env_name)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YdPG4HXnNsh",
        "colab_type": "text"
      },
      "source": [
        "## We set seeds and we get the necessary information on the states and actions in the chosen environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3RufYec_ADj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWEgDAQxnbem",
        "colab_type": "text"
      },
      "source": [
        "## We create the policy network (the Actor model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTVvG7F8_EWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "policy = TD3(state_dim, action_dim, max_action)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI60VN2Unklh",
        "colab_type": "text"
      },
      "source": [
        "## We create the Experience Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd-ZsdXR_LgV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "replay_buffer = ReplayBuffer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYOpCyiDnw7s",
        "colab_type": "text"
      },
      "source": [
        "## We define a list where all the evaluation results over 10 episodes are stored"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhC_5XJ__Orp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "cae1b1bc-4397-48a0-c457-c83e9de72ca3"
      },
      "source": [
        "evaluations = [evaluate_policy(policy)]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 9.805038\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm-4b3p6rglE",
        "colab_type": "text"
      },
      "source": [
        "## We create a new folder directory in which the final results (videos of the agent) will be populated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTL9uMd0ru03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mkdir(base, name):\n",
        "    path = os.path.join(base, name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path\n",
        "work_dir = mkdir('exp', 'brs')\n",
        "monitor_dir = mkdir(work_dir, 'monitor')\n",
        "max_episode_steps = env._max_episode_steps\n",
        "save_env_vid = False\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31n5eb03p-Fm",
        "colab_type": "text"
      },
      "source": [
        "## We initialize the variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vN5EvxK_QhT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_timesteps = 0\n",
        "timesteps_since_eval = 0\n",
        "episode_num = 0\n",
        "done = True\n",
        "t0 = time.time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9gsjvtPqLgT",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_ouY4NH_Y0I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 14093
        },
        "outputId": "5d3e39b7-006b-4ce1-eeee-51ee7081fc06"
      },
      "source": [
        "# We start the main loop over 500,000 timesteps\n",
        "while total_timesteps < max_timesteps:\n",
        "  \n",
        "  # If the episode is done\n",
        "  if done:\n",
        "\n",
        "    # If we are not at the very beginning, we start the training process of the model\n",
        "    if total_timesteps != 0:\n",
        "      print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n",
        "      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
        "\n",
        "    # We evaluate the episode and we save the policy\n",
        "    if timesteps_since_eval >= eval_freq:\n",
        "      timesteps_since_eval %= eval_freq\n",
        "      evaluations.append(evaluate_policy(policy))\n",
        "      policy.save(file_name, directory=\"./pytorch_models\")\n",
        "      np.save(\"./results/%s\" % (file_name), evaluations)\n",
        "    \n",
        "    # When the training step is done, we reset the state of the environment\n",
        "    obs = env.reset()\n",
        "    \n",
        "    # Set the Done to False\n",
        "    done = False\n",
        "    \n",
        "    # Set rewards and episode timesteps to zero\n",
        "    episode_reward = 0\n",
        "    episode_timesteps = 0\n",
        "    episode_num += 1\n",
        "  \n",
        "  # Before 10000 timesteps, we play random actions\n",
        "  if total_timesteps < start_timesteps:\n",
        "    action = env.action_space.sample()\n",
        "  else: # After 10000 timesteps, we switch to the model\n",
        "    action = policy.select_action(np.array(obs))\n",
        "    # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
        "    if expl_noise != 0:\n",
        "      action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
        "  \n",
        "  # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
        "  new_obs, reward, done, _ = env.step(action)\n",
        "  \n",
        "  # We check if the episode is done\n",
        "  done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
        "  \n",
        "  # We increase the total reward\n",
        "  episode_reward += reward\n",
        "  \n",
        "  # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
        "  replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
        "\n",
        "  # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
        "  obs = new_obs\n",
        "  episode_timesteps += 1\n",
        "  total_timesteps += 1\n",
        "  timesteps_since_eval += 1\n",
        "\n",
        "# We add the last policy evaluation to our list of evaluations and we save our model\n",
        "evaluations.append(evaluate_policy(policy))\n",
        "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
        "np.save(\"./results/%s\" % (file_name), evaluations)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Timesteps: 1000 Episode Num: 1 Reward: 498.7086827730307\n",
            "Total Timesteps: 1181 Episode Num: 2 Reward: 78.87048290211388\n",
            "Total Timesteps: 2181 Episode Num: 3 Reward: 492.0144404507873\n",
            "Total Timesteps: 3003 Episode Num: 4 Reward: 390.37169357878804\n",
            "Total Timesteps: 4003 Episode Num: 5 Reward: 477.82459902460465\n",
            "Total Timesteps: 4500 Episode Num: 6 Reward: 227.59141493962088\n",
            "Total Timesteps: 5500 Episode Num: 7 Reward: 497.81381018962134\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 87.689166\n",
            "---------------------------------------\n",
            "Total Timesteps: 6500 Episode Num: 8 Reward: 510.224048144417\n",
            "Total Timesteps: 7500 Episode Num: 9 Reward: 516.9234380706356\n",
            "Total Timesteps: 8500 Episode Num: 10 Reward: 482.1573208545452\n",
            "Total Timesteps: 8581 Episode Num: 11 Reward: 36.43488544110879\n",
            "Total Timesteps: 9581 Episode Num: 12 Reward: 480.79701810382585\n",
            "Total Timesteps: 10581 Episode Num: 13 Reward: 404.33789776171847\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 148.336648\n",
            "---------------------------------------\n",
            "Total Timesteps: 11581 Episode Num: 14 Reward: 185.1572441676608\n",
            "Total Timesteps: 12581 Episode Num: 15 Reward: 158.9551085757672\n",
            "Total Timesteps: 12604 Episode Num: 16 Reward: -4.193816313179202\n",
            "Total Timesteps: 12659 Episode Num: 17 Reward: -1.149700883758542\n",
            "Total Timesteps: 12753 Episode Num: 18 Reward: 3.7775370777853907\n",
            "Total Timesteps: 12776 Episode Num: 19 Reward: -5.4003548753876585\n",
            "Total Timesteps: 12799 Episode Num: 20 Reward: -4.8258102699117975\n",
            "Total Timesteps: 12822 Episode Num: 21 Reward: -4.69104717172807\n",
            "Total Timesteps: 12864 Episode Num: 22 Reward: -3.4894373978175066\n",
            "Total Timesteps: 12887 Episode Num: 23 Reward: -5.143515943768464\n",
            "Total Timesteps: 12910 Episode Num: 24 Reward: -4.126595797531245\n",
            "Total Timesteps: 13040 Episode Num: 25 Reward: 6.364808518527385\n",
            "Total Timesteps: 14040 Episode Num: 26 Reward: 90.4668014627773\n",
            "Total Timesteps: 15040 Episode Num: 27 Reward: 197.90495406750188\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 331.411701\n",
            "---------------------------------------\n",
            "Total Timesteps: 16040 Episode Num: 28 Reward: 265.1194667007488\n",
            "Total Timesteps: 17040 Episode Num: 29 Reward: 307.9912535434279\n",
            "Total Timesteps: 18040 Episode Num: 30 Reward: 58.11456344377017\n",
            "Total Timesteps: 19040 Episode Num: 31 Reward: 254.03989783580772\n",
            "Total Timesteps: 20040 Episode Num: 32 Reward: 257.0974758735619\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 120.853263\n",
            "---------------------------------------\n",
            "Total Timesteps: 21040 Episode Num: 33 Reward: 106.21121227052569\n",
            "Total Timesteps: 22040 Episode Num: 34 Reward: 181.6530773720785\n",
            "Total Timesteps: 23040 Episode Num: 35 Reward: 98.30468824471698\n",
            "Total Timesteps: 24040 Episode Num: 36 Reward: 521.8253124266835\n",
            "Total Timesteps: 24582 Episode Num: 37 Reward: 171.35957755600768\n",
            "Total Timesteps: 25582 Episode Num: 38 Reward: 531.751945529269\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 171.524058\n",
            "---------------------------------------\n",
            "Total Timesteps: 25734 Episode Num: 39 Reward: 13.399861164084532\n",
            "Total Timesteps: 26734 Episode Num: 40 Reward: 336.83822717106676\n",
            "Total Timesteps: 27734 Episode Num: 41 Reward: 150.88323706937538\n",
            "Total Timesteps: 28010 Episode Num: 42 Reward: 136.7200811998369\n",
            "Total Timesteps: 29010 Episode Num: 43 Reward: 369.67591147119276\n",
            "Total Timesteps: 29468 Episode Num: 44 Reward: 213.6643607606329\n",
            "Total Timesteps: 30468 Episode Num: 45 Reward: 452.7681537298358\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 114.082130\n",
            "---------------------------------------\n",
            "Total Timesteps: 30634 Episode Num: 46 Reward: 54.414380155495095\n",
            "Total Timesteps: 31634 Episode Num: 47 Reward: 349.7349990669002\n",
            "Total Timesteps: 32634 Episode Num: 48 Reward: 428.4097524541163\n",
            "Total Timesteps: 33634 Episode Num: 49 Reward: 408.68248457441274\n",
            "Total Timesteps: 34634 Episode Num: 50 Reward: 267.6600200367911\n",
            "Total Timesteps: 35634 Episode Num: 51 Reward: 364.46252094489444\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 678.318316\n",
            "---------------------------------------\n",
            "Total Timesteps: 36634 Episode Num: 52 Reward: 463.07490663172666\n",
            "Total Timesteps: 37634 Episode Num: 53 Reward: 517.543353485846\n",
            "Total Timesteps: 38634 Episode Num: 54 Reward: 422.3885597839847\n",
            "Total Timesteps: 39634 Episode Num: 55 Reward: 441.28226708296035\n",
            "Total Timesteps: 40634 Episode Num: 56 Reward: 434.63503218731694\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 459.023909\n",
            "---------------------------------------\n",
            "Total Timesteps: 41634 Episode Num: 57 Reward: 617.3038806059448\n",
            "Total Timesteps: 42634 Episode Num: 58 Reward: 654.7635657787066\n",
            "Total Timesteps: 43634 Episode Num: 59 Reward: 611.6404305007791\n",
            "Total Timesteps: 44634 Episode Num: 60 Reward: 672.0431282231855\n",
            "Total Timesteps: 45634 Episode Num: 61 Reward: 303.9556713312232\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 448.975840\n",
            "---------------------------------------\n",
            "Total Timesteps: 46634 Episode Num: 62 Reward: 519.9902482609996\n",
            "Total Timesteps: 47634 Episode Num: 63 Reward: 493.51342817671195\n",
            "Total Timesteps: 48634 Episode Num: 64 Reward: 511.8486557896573\n",
            "Total Timesteps: 49634 Episode Num: 65 Reward: 323.40366307864196\n",
            "Total Timesteps: 50634 Episode Num: 66 Reward: 400.17559463085416\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 230.030833\n",
            "---------------------------------------\n",
            "Total Timesteps: 51634 Episode Num: 67 Reward: 198.44823556350508\n",
            "Total Timesteps: 52634 Episode Num: 68 Reward: 229.83968262781318\n",
            "Total Timesteps: 53634 Episode Num: 69 Reward: 527.8189338369451\n",
            "Total Timesteps: 54634 Episode Num: 70 Reward: 396.90129316290427\n",
            "Total Timesteps: 55634 Episode Num: 71 Reward: 320.9273851421874\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 386.928310\n",
            "---------------------------------------\n",
            "Total Timesteps: 55753 Episode Num: 72 Reward: -5.051111051807785\n",
            "Total Timesteps: 56753 Episode Num: 73 Reward: 398.7660649822879\n",
            "Total Timesteps: 57753 Episode Num: 74 Reward: 452.9311479440855\n",
            "Total Timesteps: 58753 Episode Num: 75 Reward: 344.9581431806491\n",
            "Total Timesteps: 59753 Episode Num: 76 Reward: 356.8526238464033\n",
            "Total Timesteps: 60753 Episode Num: 77 Reward: 555.876535080144\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 427.327238\n",
            "---------------------------------------\n",
            "Total Timesteps: 61753 Episode Num: 78 Reward: 527.3836170738881\n",
            "Total Timesteps: 62753 Episode Num: 79 Reward: 487.8075045409197\n",
            "Total Timesteps: 62775 Episode Num: 80 Reward: 0.667436202930098\n",
            "Total Timesteps: 62796 Episode Num: 81 Reward: -0.22869258091176814\n",
            "Total Timesteps: 62817 Episode Num: 82 Reward: -1.9508767217541327\n",
            "Total Timesteps: 62841 Episode Num: 83 Reward: -0.02668534425017377\n",
            "Total Timesteps: 62869 Episode Num: 84 Reward: 1.9853227297039906\n",
            "Total Timesteps: 63869 Episode Num: 85 Reward: 432.8319252708427\n",
            "Total Timesteps: 64869 Episode Num: 86 Reward: 501.3856856744039\n",
            "Total Timesteps: 65869 Episode Num: 87 Reward: 295.14175304271754\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 338.905414\n",
            "---------------------------------------\n",
            "Total Timesteps: 66105 Episode Num: 88 Reward: 67.33748671809361\n",
            "Total Timesteps: 67105 Episode Num: 89 Reward: 331.879342273409\n",
            "Total Timesteps: 68105 Episode Num: 90 Reward: 596.6913673234451\n",
            "Total Timesteps: 69105 Episode Num: 91 Reward: 460.9307592378999\n",
            "Total Timesteps: 70105 Episode Num: 92 Reward: 632.6235127513642\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 492.482744\n",
            "---------------------------------------\n",
            "Total Timesteps: 71105 Episode Num: 93 Reward: 466.9165068378695\n",
            "Total Timesteps: 72105 Episode Num: 94 Reward: 406.78740567408767\n",
            "Total Timesteps: 73105 Episode Num: 95 Reward: 388.7475544457742\n",
            "Total Timesteps: 74105 Episode Num: 96 Reward: 396.69092129108753\n",
            "Total Timesteps: 75105 Episode Num: 97 Reward: 400.86552817603615\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 390.633861\n",
            "---------------------------------------\n",
            "Total Timesteps: 76105 Episode Num: 98 Reward: 350.4510738514903\n",
            "Total Timesteps: 77105 Episode Num: 99 Reward: 316.0789918129974\n",
            "Total Timesteps: 78105 Episode Num: 100 Reward: 564.4133045055537\n",
            "Total Timesteps: 79105 Episode Num: 101 Reward: 290.94861994341727\n",
            "Total Timesteps: 80105 Episode Num: 102 Reward: 399.5591941109714\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 397.649412\n",
            "---------------------------------------\n",
            "Total Timesteps: 81105 Episode Num: 103 Reward: 472.304662583608\n",
            "Total Timesteps: 82105 Episode Num: 104 Reward: 335.62091517502654\n",
            "Total Timesteps: 83105 Episode Num: 105 Reward: 351.35795632452334\n",
            "Total Timesteps: 84105 Episode Num: 106 Reward: 592.5026846790046\n",
            "Total Timesteps: 85105 Episode Num: 107 Reward: 634.4959095609638\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 459.839906\n",
            "---------------------------------------\n",
            "Total Timesteps: 86105 Episode Num: 108 Reward: 659.3040968054135\n",
            "Total Timesteps: 87105 Episode Num: 109 Reward: 328.66655831553254\n",
            "Total Timesteps: 88105 Episode Num: 110 Reward: 593.6877056887099\n",
            "Total Timesteps: 89105 Episode Num: 111 Reward: 333.92284821377024\n",
            "Total Timesteps: 90105 Episode Num: 112 Reward: 427.6811595204129\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 418.657587\n",
            "---------------------------------------\n",
            "Total Timesteps: 91105 Episode Num: 113 Reward: 378.292729684922\n",
            "Total Timesteps: 91445 Episode Num: 114 Reward: 173.19492832392513\n",
            "Total Timesteps: 92445 Episode Num: 115 Reward: 539.9762249227407\n",
            "Total Timesteps: 92658 Episode Num: 116 Reward: 139.5158020502375\n",
            "Total Timesteps: 93658 Episode Num: 117 Reward: 420.88632299252663\n",
            "Total Timesteps: 94658 Episode Num: 118 Reward: 374.24149340327534\n",
            "Total Timesteps: 95044 Episode Num: 119 Reward: 142.9450132580183\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 413.447288\n",
            "---------------------------------------\n",
            "Total Timesteps: 95245 Episode Num: 120 Reward: 26.748731581261303\n",
            "Total Timesteps: 96245 Episode Num: 121 Reward: 254.1152071457635\n",
            "Total Timesteps: 97245 Episode Num: 122 Reward: 415.4028988829471\n",
            "Total Timesteps: 98245 Episode Num: 123 Reward: 489.98933251639045\n",
            "Total Timesteps: 99245 Episode Num: 124 Reward: 121.1883409460957\n",
            "Total Timesteps: 100245 Episode Num: 125 Reward: 310.71067468241495\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 304.989528\n",
            "---------------------------------------\n",
            "Total Timesteps: 101245 Episode Num: 126 Reward: 330.4192663121165\n",
            "Total Timesteps: 102245 Episode Num: 127 Reward: 503.62000665401683\n",
            "Total Timesteps: 102415 Episode Num: 128 Reward: -1.3990151574961234\n",
            "Total Timesteps: 103415 Episode Num: 129 Reward: 266.18877479110586\n",
            "Total Timesteps: 103523 Episode Num: 130 Reward: 88.53397529553928\n",
            "Total Timesteps: 104523 Episode Num: 131 Reward: 413.1208949847231\n",
            "Total Timesteps: 105523 Episode Num: 132 Reward: 374.06167299065663\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 325.800042\n",
            "---------------------------------------\n",
            "Total Timesteps: 106523 Episode Num: 133 Reward: 326.20519106387644\n",
            "Total Timesteps: 107523 Episode Num: 134 Reward: 501.6120762329664\n",
            "Total Timesteps: 108523 Episode Num: 135 Reward: 423.10683741403034\n",
            "Total Timesteps: 109523 Episode Num: 136 Reward: 583.5783801641003\n",
            "Total Timesteps: 110523 Episode Num: 137 Reward: 611.4007299837754\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 248.395097\n",
            "---------------------------------------\n",
            "Total Timesteps: 110667 Episode Num: 138 Reward: 51.83352009222487\n",
            "Total Timesteps: 111667 Episode Num: 139 Reward: 223.72247246658065\n",
            "Total Timesteps: 112667 Episode Num: 140 Reward: 551.2405304392406\n",
            "Total Timesteps: 113667 Episode Num: 141 Reward: 652.5116648861143\n",
            "Total Timesteps: 114667 Episode Num: 142 Reward: 607.0894479440867\n",
            "Total Timesteps: 115667 Episode Num: 143 Reward: 573.3677091353418\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 627.322978\n",
            "---------------------------------------\n",
            "Total Timesteps: 116667 Episode Num: 144 Reward: 622.8046007213436\n",
            "Total Timesteps: 117667 Episode Num: 145 Reward: 238.33074434328165\n",
            "Total Timesteps: 118667 Episode Num: 146 Reward: 662.4439256410366\n",
            "Total Timesteps: 119667 Episode Num: 147 Reward: 447.14251575709125\n",
            "Total Timesteps: 120667 Episode Num: 148 Reward: 493.19069846075837\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 486.871199\n",
            "---------------------------------------\n",
            "Total Timesteps: 121667 Episode Num: 149 Reward: 549.1534867993216\n",
            "Total Timesteps: 122667 Episode Num: 150 Reward: 526.8670262305525\n",
            "Total Timesteps: 123667 Episode Num: 151 Reward: 575.3573886636763\n",
            "Total Timesteps: 124667 Episode Num: 152 Reward: 683.0662286441474\n",
            "Total Timesteps: 125667 Episode Num: 153 Reward: 632.2999388275965\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 515.257593\n",
            "---------------------------------------\n",
            "Total Timesteps: 126667 Episode Num: 154 Reward: 563.3117998265395\n",
            "Total Timesteps: 127667 Episode Num: 155 Reward: 369.31581614736496\n",
            "Total Timesteps: 128667 Episode Num: 156 Reward: 336.04937216598876\n",
            "Total Timesteps: 129667 Episode Num: 157 Reward: 272.9649622658604\n",
            "Total Timesteps: 130667 Episode Num: 158 Reward: 384.14342477278007\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 474.859520\n",
            "---------------------------------------\n",
            "Total Timesteps: 131667 Episode Num: 159 Reward: 498.82992320932584\n",
            "Total Timesteps: 132667 Episode Num: 160 Reward: 438.48286466298447\n",
            "Total Timesteps: 133667 Episode Num: 161 Reward: 803.3868866079408\n",
            "Total Timesteps: 134667 Episode Num: 162 Reward: 445.6040872859268\n",
            "Total Timesteps: 135667 Episode Num: 163 Reward: 226.02389746246072\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 409.727764\n",
            "---------------------------------------\n",
            "Total Timesteps: 136667 Episode Num: 164 Reward: 350.81955737337586\n",
            "Total Timesteps: 137667 Episode Num: 165 Reward: 389.9179360145917\n",
            "Total Timesteps: 138667 Episode Num: 166 Reward: 449.595633657888\n",
            "Total Timesteps: 139667 Episode Num: 167 Reward: 436.83595831114235\n",
            "Total Timesteps: 140667 Episode Num: 168 Reward: 659.2963616064256\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 539.103046\n",
            "---------------------------------------\n",
            "Total Timesteps: 141667 Episode Num: 169 Reward: 453.1291919738895\n",
            "Total Timesteps: 142667 Episode Num: 170 Reward: 465.20912822006636\n",
            "Total Timesteps: 143667 Episode Num: 171 Reward: 603.9860792806078\n",
            "Total Timesteps: 144667 Episode Num: 172 Reward: 576.0092814048433\n",
            "Total Timesteps: 145667 Episode Num: 173 Reward: 546.0024458368782\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 460.070261\n",
            "---------------------------------------\n",
            "Total Timesteps: 146667 Episode Num: 174 Reward: 545.8668060701715\n",
            "Total Timesteps: 147667 Episode Num: 175 Reward: 635.0741192419331\n",
            "Total Timesteps: 148667 Episode Num: 176 Reward: 619.9237502649605\n",
            "Total Timesteps: 149667 Episode Num: 177 Reward: 588.5998249631906\n",
            "Total Timesteps: 150667 Episode Num: 178 Reward: 652.8847865671269\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 634.550112\n",
            "---------------------------------------\n",
            "Total Timesteps: 151667 Episode Num: 179 Reward: 654.5953809322609\n",
            "Total Timesteps: 152667 Episode Num: 180 Reward: 682.6138788785979\n",
            "Total Timesteps: 153667 Episode Num: 181 Reward: 642.1665983638989\n",
            "Total Timesteps: 154667 Episode Num: 182 Reward: 691.2568344786695\n",
            "Total Timesteps: 155667 Episode Num: 183 Reward: 668.8774612526453\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 661.915449\n",
            "---------------------------------------\n",
            "Total Timesteps: 156667 Episode Num: 184 Reward: 673.1707886891744\n",
            "Total Timesteps: 157667 Episode Num: 185 Reward: 723.8880522982114\n",
            "Total Timesteps: 158667 Episode Num: 186 Reward: 674.9696449018705\n",
            "Total Timesteps: 159667 Episode Num: 187 Reward: 515.0186738214096\n",
            "Total Timesteps: 160667 Episode Num: 188 Reward: 729.8721772004427\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 637.522810\n",
            "---------------------------------------\n",
            "Total Timesteps: 161667 Episode Num: 189 Reward: 659.2950477800655\n",
            "Total Timesteps: 162667 Episode Num: 190 Reward: 789.2166043570979\n",
            "Total Timesteps: 163667 Episode Num: 191 Reward: 741.7355481406379\n",
            "Total Timesteps: 164667 Episode Num: 192 Reward: 667.5735806391059\n",
            "Total Timesteps: 165667 Episode Num: 193 Reward: 686.1125431376311\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 646.068148\n",
            "---------------------------------------\n",
            "Total Timesteps: 166667 Episode Num: 194 Reward: 790.6942838362187\n",
            "Total Timesteps: 167667 Episode Num: 195 Reward: 543.2293965035875\n",
            "Total Timesteps: 168667 Episode Num: 196 Reward: 671.1073582131492\n",
            "Total Timesteps: 169667 Episode Num: 197 Reward: 796.5435747552757\n",
            "Total Timesteps: 170667 Episode Num: 198 Reward: 805.8543267375601\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 723.167499\n",
            "---------------------------------------\n",
            "Total Timesteps: 171667 Episode Num: 199 Reward: 872.4342723354807\n",
            "Total Timesteps: 172667 Episode Num: 200 Reward: 624.0145911114316\n",
            "Total Timesteps: 173667 Episode Num: 201 Reward: 604.2480846132687\n",
            "Total Timesteps: 174667 Episode Num: 202 Reward: 646.30450357719\n",
            "Total Timesteps: 175667 Episode Num: 203 Reward: 570.0355401271685\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 685.664902\n",
            "---------------------------------------\n",
            "Total Timesteps: 176667 Episode Num: 204 Reward: 663.2527834757229\n",
            "Total Timesteps: 177667 Episode Num: 205 Reward: 458.508330364546\n",
            "Total Timesteps: 178667 Episode Num: 206 Reward: 700.7443367479543\n",
            "Total Timesteps: 179667 Episode Num: 207 Reward: 736.6437085175686\n",
            "Total Timesteps: 180667 Episode Num: 208 Reward: 659.3031861028935\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 663.796209\n",
            "---------------------------------------\n",
            "Total Timesteps: 181667 Episode Num: 209 Reward: 678.1683520817481\n",
            "Total Timesteps: 182667 Episode Num: 210 Reward: 603.8094201042383\n",
            "Total Timesteps: 183667 Episode Num: 211 Reward: 752.8872783597961\n",
            "Total Timesteps: 184667 Episode Num: 212 Reward: 506.90492992914574\n",
            "Total Timesteps: 185667 Episode Num: 213 Reward: 582.9153396540519\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 720.876631\n",
            "---------------------------------------\n",
            "Total Timesteps: 186667 Episode Num: 214 Reward: 765.6999754257657\n",
            "Total Timesteps: 187667 Episode Num: 215 Reward: 742.8718614419681\n",
            "Total Timesteps: 188667 Episode Num: 216 Reward: 756.6711946369433\n",
            "Total Timesteps: 189667 Episode Num: 217 Reward: 874.8327137982053\n",
            "Total Timesteps: 190667 Episode Num: 218 Reward: 1079.9720044411583\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 776.983028\n",
            "---------------------------------------\n",
            "Total Timesteps: 191667 Episode Num: 219 Reward: 689.8596980354986\n",
            "Total Timesteps: 192667 Episode Num: 220 Reward: 743.6566140218591\n",
            "Total Timesteps: 193667 Episode Num: 221 Reward: 881.7580060303173\n",
            "Total Timesteps: 194667 Episode Num: 222 Reward: 580.3518208039129\n",
            "Total Timesteps: 195667 Episode Num: 223 Reward: 567.6663433768093\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 730.466653\n",
            "---------------------------------------\n",
            "Total Timesteps: 196667 Episode Num: 224 Reward: 809.8935660020951\n",
            "Total Timesteps: 197667 Episode Num: 225 Reward: 862.4503202948081\n",
            "Total Timesteps: 198667 Episode Num: 226 Reward: 491.9821369761933\n",
            "Total Timesteps: 199667 Episode Num: 227 Reward: 432.46550874487457\n",
            "Total Timesteps: 200667 Episode Num: 228 Reward: 339.1625371272823\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 695.696633\n",
            "---------------------------------------\n",
            "Total Timesteps: 201667 Episode Num: 229 Reward: 746.5519599610717\n",
            "Total Timesteps: 202667 Episode Num: 230 Reward: 722.354981669851\n",
            "Total Timesteps: 203667 Episode Num: 231 Reward: 432.22975278427214\n",
            "Total Timesteps: 204667 Episode Num: 232 Reward: 654.1076425922371\n",
            "Total Timesteps: 205667 Episode Num: 233 Reward: 614.5784377569869\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 585.939208\n",
            "---------------------------------------\n",
            "Total Timesteps: 206667 Episode Num: 234 Reward: 387.1670887695024\n",
            "Total Timesteps: 207667 Episode Num: 235 Reward: 596.2297384271666\n",
            "Total Timesteps: 208667 Episode Num: 236 Reward: 566.8517324294295\n",
            "Total Timesteps: 208905 Episode Num: 237 Reward: 124.1405850314661\n",
            "Total Timesteps: 209905 Episode Num: 238 Reward: 365.2791917088394\n",
            "Total Timesteps: 210905 Episode Num: 239 Reward: 664.6876284383368\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 560.550714\n",
            "---------------------------------------\n",
            "Total Timesteps: 211905 Episode Num: 240 Reward: 493.14692417181936\n",
            "Total Timesteps: 212905 Episode Num: 241 Reward: 1090.7800437139142\n",
            "Total Timesteps: 213905 Episode Num: 242 Reward: 720.9004593502033\n",
            "Total Timesteps: 214905 Episode Num: 243 Reward: 834.4502774974301\n",
            "Total Timesteps: 215905 Episode Num: 244 Reward: 989.2761989308104\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 732.818033\n",
            "---------------------------------------\n",
            "Total Timesteps: 216905 Episode Num: 245 Reward: 642.4007718372611\n",
            "Total Timesteps: 217905 Episode Num: 246 Reward: 871.5312207516567\n",
            "Total Timesteps: 218905 Episode Num: 247 Reward: 504.0672646756063\n",
            "Total Timesteps: 219905 Episode Num: 248 Reward: 673.2517132911707\n",
            "Total Timesteps: 220905 Episode Num: 249 Reward: 529.3215517546126\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 821.963046\n",
            "---------------------------------------\n",
            "Total Timesteps: 221905 Episode Num: 250 Reward: 1131.2969759519979\n",
            "Total Timesteps: 222905 Episode Num: 251 Reward: 1093.908411314947\n",
            "Total Timesteps: 223905 Episode Num: 252 Reward: 783.3076548473831\n",
            "Total Timesteps: 224905 Episode Num: 253 Reward: 548.5586003000666\n",
            "Total Timesteps: 225905 Episode Num: 254 Reward: 755.8460680232563\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 958.402537\n",
            "---------------------------------------\n",
            "Total Timesteps: 226905 Episode Num: 255 Reward: 1145.300542139557\n",
            "Total Timesteps: 227905 Episode Num: 256 Reward: 626.707042337026\n",
            "Total Timesteps: 228905 Episode Num: 257 Reward: 924.6915711103569\n",
            "Total Timesteps: 229905 Episode Num: 258 Reward: 883.6700530130762\n",
            "Total Timesteps: 230905 Episode Num: 259 Reward: 847.768565276842\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 784.195562\n",
            "---------------------------------------\n",
            "Total Timesteps: 231905 Episode Num: 260 Reward: 1036.899839456853\n",
            "Total Timesteps: 232905 Episode Num: 261 Reward: 1066.1759435883482\n",
            "Total Timesteps: 233905 Episode Num: 262 Reward: 766.3558297586767\n",
            "Total Timesteps: 234905 Episode Num: 263 Reward: 898.6090525791778\n",
            "Total Timesteps: 235905 Episode Num: 264 Reward: 942.8479119664652\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 799.451342\n",
            "---------------------------------------\n",
            "Total Timesteps: 236905 Episode Num: 265 Reward: 762.6621444406414\n",
            "Total Timesteps: 237905 Episode Num: 266 Reward: 905.2734142704992\n",
            "Total Timesteps: 238905 Episode Num: 267 Reward: 542.7199893933088\n",
            "Total Timesteps: 239905 Episode Num: 268 Reward: 681.6858558400761\n",
            "Total Timesteps: 240905 Episode Num: 269 Reward: 666.6536884087707\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 670.601880\n",
            "---------------------------------------\n",
            "Total Timesteps: 241905 Episode Num: 270 Reward: 790.0041285830439\n",
            "Total Timesteps: 242905 Episode Num: 271 Reward: 791.9019781163124\n",
            "Total Timesteps: 243905 Episode Num: 272 Reward: 857.5237189122082\n",
            "Total Timesteps: 244905 Episode Num: 273 Reward: 826.0963547404735\n",
            "Total Timesteps: 245905 Episode Num: 274 Reward: 1157.090428865298\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1178.929703\n",
            "---------------------------------------\n",
            "Total Timesteps: 246905 Episode Num: 275 Reward: 899.2104800887787\n",
            "Total Timesteps: 247905 Episode Num: 276 Reward: 1415.9168679963395\n",
            "Total Timesteps: 248905 Episode Num: 277 Reward: 1448.1639165862853\n",
            "Total Timesteps: 249905 Episode Num: 278 Reward: 705.3406117132921\n",
            "Total Timesteps: 250905 Episode Num: 279 Reward: 1059.7420756185118\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 840.472138\n",
            "---------------------------------------\n",
            "Total Timesteps: 251905 Episode Num: 280 Reward: 796.2399715435675\n",
            "Total Timesteps: 252905 Episode Num: 281 Reward: 981.1539545350167\n",
            "Total Timesteps: 253905 Episode Num: 282 Reward: 1313.4527490357937\n",
            "Total Timesteps: 254905 Episode Num: 283 Reward: 687.8285095500478\n",
            "Total Timesteps: 255905 Episode Num: 284 Reward: 683.1658596711843\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 827.030430\n",
            "---------------------------------------\n",
            "Total Timesteps: 256905 Episode Num: 285 Reward: 1314.3122520952231\n",
            "Total Timesteps: 257905 Episode Num: 286 Reward: 1330.7173974626244\n",
            "Total Timesteps: 258905 Episode Num: 287 Reward: 1237.110793580027\n",
            "Total Timesteps: 259905 Episode Num: 288 Reward: 1403.8702488294136\n",
            "Total Timesteps: 260905 Episode Num: 289 Reward: 1393.5727247269128\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1069.287921\n",
            "---------------------------------------\n",
            "Total Timesteps: 261905 Episode Num: 290 Reward: 880.7582531732247\n",
            "Total Timesteps: 262905 Episode Num: 291 Reward: 788.4559882728065\n",
            "Total Timesteps: 263905 Episode Num: 292 Reward: 878.2178374238902\n",
            "Total Timesteps: 264905 Episode Num: 293 Reward: 711.6332317503376\n",
            "Total Timesteps: 265905 Episode Num: 294 Reward: 545.4145279901657\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1139.349143\n",
            "---------------------------------------\n",
            "Total Timesteps: 266905 Episode Num: 295 Reward: 714.8277851538679\n",
            "Total Timesteps: 267905 Episode Num: 296 Reward: 1187.5103103250012\n",
            "Total Timesteps: 268905 Episode Num: 297 Reward: 1067.308288043462\n",
            "Total Timesteps: 269905 Episode Num: 298 Reward: 1254.797334737097\n",
            "Total Timesteps: 270905 Episode Num: 299 Reward: 1428.9439313454225\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1057.230512\n",
            "---------------------------------------\n",
            "Total Timesteps: 271905 Episode Num: 300 Reward: 592.9995823129349\n",
            "Total Timesteps: 272905 Episode Num: 301 Reward: 1124.0738317420166\n",
            "Total Timesteps: 273905 Episode Num: 302 Reward: 1344.4828183705524\n",
            "Total Timesteps: 274905 Episode Num: 303 Reward: 973.084724749543\n",
            "Total Timesteps: 275905 Episode Num: 304 Reward: 1263.5888014664774\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1367.902809\n",
            "---------------------------------------\n",
            "Total Timesteps: 276905 Episode Num: 305 Reward: 864.0649081291933\n",
            "Total Timesteps: 277905 Episode Num: 306 Reward: 1364.089420135368\n",
            "Total Timesteps: 278905 Episode Num: 307 Reward: 1514.9050401731272\n",
            "Total Timesteps: 279905 Episode Num: 308 Reward: 1513.2632148047367\n",
            "Total Timesteps: 280905 Episode Num: 309 Reward: 1491.58255685704\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1487.530939\n",
            "---------------------------------------\n",
            "Total Timesteps: 281905 Episode Num: 310 Reward: 1503.1983808698653\n",
            "Total Timesteps: 282905 Episode Num: 311 Reward: 1282.4243252357878\n",
            "Total Timesteps: 283905 Episode Num: 312 Reward: 1446.8628066388465\n",
            "Total Timesteps: 284905 Episode Num: 313 Reward: 1378.6360630300398\n",
            "Total Timesteps: 285905 Episode Num: 314 Reward: 1482.3445715728212\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1330.651426\n",
            "---------------------------------------\n",
            "Total Timesteps: 286905 Episode Num: 315 Reward: 1381.6887234411313\n",
            "Total Timesteps: 287905 Episode Num: 316 Reward: 1546.3706592112587\n",
            "Total Timesteps: 288905 Episode Num: 317 Reward: 1373.582099346969\n",
            "Total Timesteps: 289905 Episode Num: 318 Reward: 1332.2589793366064\n",
            "Total Timesteps: 290905 Episode Num: 319 Reward: 1386.0734573648344\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1427.191090\n",
            "---------------------------------------\n",
            "Total Timesteps: 291905 Episode Num: 320 Reward: 1397.6371918875127\n",
            "Total Timesteps: 292905 Episode Num: 321 Reward: 1457.9922086809888\n",
            "Total Timesteps: 293905 Episode Num: 322 Reward: 1544.7693315997851\n",
            "Total Timesteps: 294905 Episode Num: 323 Reward: 1543.464215656157\n",
            "Total Timesteps: 295905 Episode Num: 324 Reward: 1595.087482720854\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1013.218779\n",
            "---------------------------------------\n",
            "Total Timesteps: 296905 Episode Num: 325 Reward: 1580.219203848432\n",
            "Total Timesteps: 297905 Episode Num: 326 Reward: 1677.3712027320137\n",
            "Total Timesteps: 298905 Episode Num: 327 Reward: 1662.981656128902\n",
            "Total Timesteps: 299905 Episode Num: 328 Reward: 1763.5114191620987\n",
            "Total Timesteps: 300905 Episode Num: 329 Reward: 1564.8237263031626\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1541.100666\n",
            "---------------------------------------\n",
            "Total Timesteps: 301905 Episode Num: 330 Reward: 1867.088582292788\n",
            "Total Timesteps: 302905 Episode Num: 331 Reward: 1346.8550384306134\n",
            "Total Timesteps: 303905 Episode Num: 332 Reward: 711.428888389834\n",
            "Total Timesteps: 304905 Episode Num: 333 Reward: 1623.5813182403504\n",
            "Total Timesteps: 305905 Episode Num: 334 Reward: 669.9992599776219\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1311.318945\n",
            "---------------------------------------\n",
            "Total Timesteps: 306905 Episode Num: 335 Reward: 1631.0769547806467\n",
            "Total Timesteps: 307905 Episode Num: 336 Reward: 1646.9648095106584\n",
            "Total Timesteps: 308905 Episode Num: 337 Reward: 1855.1274191628079\n",
            "Total Timesteps: 309905 Episode Num: 338 Reward: 1795.79417552418\n",
            "Total Timesteps: 310905 Episode Num: 339 Reward: 1964.0053678924319\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1859.425454\n",
            "---------------------------------------\n",
            "Total Timesteps: 311905 Episode Num: 340 Reward: 1846.6695323375118\n",
            "Total Timesteps: 312905 Episode Num: 341 Reward: 1390.6730933604983\n",
            "Total Timesteps: 313905 Episode Num: 342 Reward: 1753.542635442716\n",
            "Total Timesteps: 314905 Episode Num: 343 Reward: 1880.6575552967688\n",
            "Total Timesteps: 315905 Episode Num: 344 Reward: 1777.7536434487429\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1575.439575\n",
            "---------------------------------------\n",
            "Total Timesteps: 316905 Episode Num: 345 Reward: 1637.2347543365088\n",
            "Total Timesteps: 317905 Episode Num: 346 Reward: 1727.5775229041067\n",
            "Total Timesteps: 318905 Episode Num: 347 Reward: 1735.7899127139624\n",
            "Total Timesteps: 319905 Episode Num: 348 Reward: 1636.4326740657857\n",
            "Total Timesteps: 320905 Episode Num: 349 Reward: 1781.4185330937623\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1799.708650\n",
            "---------------------------------------\n",
            "Total Timesteps: 321905 Episode Num: 350 Reward: 1718.565333086903\n",
            "Total Timesteps: 322905 Episode Num: 351 Reward: 1733.22174855246\n",
            "Total Timesteps: 323905 Episode Num: 352 Reward: 1859.3452205453427\n",
            "Total Timesteps: 324905 Episode Num: 353 Reward: 1957.198549852431\n",
            "Total Timesteps: 325905 Episode Num: 354 Reward: 1955.0077508089314\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1903.133158\n",
            "---------------------------------------\n",
            "Total Timesteps: 326905 Episode Num: 355 Reward: 1819.463341055711\n",
            "Total Timesteps: 327905 Episode Num: 356 Reward: 1885.120737504927\n",
            "Total Timesteps: 328905 Episode Num: 357 Reward: 1858.2148088856188\n",
            "Total Timesteps: 329905 Episode Num: 358 Reward: 1986.3941149861716\n",
            "Total Timesteps: 330905 Episode Num: 359 Reward: 1953.0408014271118\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1895.909758\n",
            "---------------------------------------\n",
            "Total Timesteps: 331905 Episode Num: 360 Reward: 1919.9829861522012\n",
            "Total Timesteps: 332905 Episode Num: 361 Reward: 1971.687324420108\n",
            "Total Timesteps: 333905 Episode Num: 362 Reward: 1849.8883486982486\n",
            "Total Timesteps: 334905 Episode Num: 363 Reward: 1990.275370396802\n",
            "Total Timesteps: 335905 Episode Num: 364 Reward: 1915.9219801613378\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1937.053613\n",
            "---------------------------------------\n",
            "Total Timesteps: 336905 Episode Num: 365 Reward: 1825.540875543859\n",
            "Total Timesteps: 337905 Episode Num: 366 Reward: 1872.7444306351586\n",
            "Total Timesteps: 338905 Episode Num: 367 Reward: 1917.3340138868377\n",
            "Total Timesteps: 339905 Episode Num: 368 Reward: 1867.9335026612002\n",
            "Total Timesteps: 340905 Episode Num: 369 Reward: 1957.997766702154\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2000.571117\n",
            "---------------------------------------\n",
            "Total Timesteps: 341905 Episode Num: 370 Reward: 1962.7524865498415\n",
            "Total Timesteps: 342905 Episode Num: 371 Reward: 2032.3712214658146\n",
            "Total Timesteps: 343905 Episode Num: 372 Reward: 1970.323793225442\n",
            "Total Timesteps: 344905 Episode Num: 373 Reward: 1790.7656271406468\n",
            "Total Timesteps: 345905 Episode Num: 374 Reward: 1975.0078526101775\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1992.385203\n",
            "---------------------------------------\n",
            "Total Timesteps: 346905 Episode Num: 375 Reward: 1935.7266098990444\n",
            "Total Timesteps: 347905 Episode Num: 376 Reward: 1851.9652607562257\n",
            "Total Timesteps: 348905 Episode Num: 377 Reward: 2031.8766518486639\n",
            "Total Timesteps: 349905 Episode Num: 378 Reward: 2097.692361537138\n",
            "Total Timesteps: 350905 Episode Num: 379 Reward: 2067.6494472985564\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2044.142838\n",
            "---------------------------------------\n",
            "Total Timesteps: 351905 Episode Num: 380 Reward: 1978.163285246381\n",
            "Total Timesteps: 352905 Episode Num: 381 Reward: 2099.957883412024\n",
            "Total Timesteps: 353905 Episode Num: 382 Reward: 2083.9975770633346\n",
            "Total Timesteps: 354905 Episode Num: 383 Reward: 2016.8347973702594\n",
            "Total Timesteps: 355905 Episode Num: 384 Reward: 1930.186947229638\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2051.686195\n",
            "---------------------------------------\n",
            "Total Timesteps: 356905 Episode Num: 385 Reward: 2010.1432952082787\n",
            "Total Timesteps: 357905 Episode Num: 386 Reward: 2062.639214099757\n",
            "Total Timesteps: 358905 Episode Num: 387 Reward: 2082.066826882534\n",
            "Total Timesteps: 359905 Episode Num: 388 Reward: 1973.6873320477853\n",
            "Total Timesteps: 360905 Episode Num: 389 Reward: 2099.4885322250416\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2125.912728\n",
            "---------------------------------------\n",
            "Total Timesteps: 361905 Episode Num: 390 Reward: 2032.5480728135835\n",
            "Total Timesteps: 362905 Episode Num: 391 Reward: 2134.980458686659\n",
            "Total Timesteps: 363905 Episode Num: 392 Reward: 2121.8074860040133\n",
            "Total Timesteps: 364905 Episode Num: 393 Reward: 1968.6341190339635\n",
            "Total Timesteps: 365905 Episode Num: 394 Reward: 2092.4197176436187\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2042.929957\n",
            "---------------------------------------\n",
            "Total Timesteps: 366905 Episode Num: 395 Reward: 2070.6883130064516\n",
            "Total Timesteps: 367905 Episode Num: 396 Reward: 1997.193560634754\n",
            "Total Timesteps: 368905 Episode Num: 397 Reward: 2074.07483581275\n",
            "Total Timesteps: 369905 Episode Num: 398 Reward: 2015.7519279866588\n",
            "Total Timesteps: 370905 Episode Num: 399 Reward: 2003.3831903840473\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2094.657167\n",
            "---------------------------------------\n",
            "Total Timesteps: 371905 Episode Num: 400 Reward: 2065.055791149596\n",
            "Total Timesteps: 372905 Episode Num: 401 Reward: 1961.7500968763297\n",
            "Total Timesteps: 373905 Episode Num: 402 Reward: 2067.172538544964\n",
            "Total Timesteps: 374905 Episode Num: 403 Reward: 1964.8798633456347\n",
            "Total Timesteps: 375905 Episode Num: 404 Reward: 2161.5897420706024\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2173.891002\n",
            "---------------------------------------\n",
            "Total Timesteps: 376905 Episode Num: 405 Reward: 2168.221509256949\n",
            "Total Timesteps: 377905 Episode Num: 406 Reward: 2107.759693872506\n",
            "Total Timesteps: 378905 Episode Num: 407 Reward: 2082.1831692753453\n",
            "Total Timesteps: 379905 Episode Num: 408 Reward: 2051.1356653124776\n",
            "Total Timesteps: 380905 Episode Num: 409 Reward: 2226.8771221175025\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2120.414851\n",
            "---------------------------------------\n",
            "Total Timesteps: 381905 Episode Num: 410 Reward: 2038.5217970765623\n",
            "Total Timesteps: 382905 Episode Num: 411 Reward: 2120.457028065565\n",
            "Total Timesteps: 383905 Episode Num: 412 Reward: 2115.9481310484452\n",
            "Total Timesteps: 384905 Episode Num: 413 Reward: 2152.382163699869\n",
            "Total Timesteps: 385905 Episode Num: 414 Reward: 2145.4123202202904\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2144.070169\n",
            "---------------------------------------\n",
            "Total Timesteps: 386905 Episode Num: 415 Reward: 2155.5325272324776\n",
            "Total Timesteps: 387905 Episode Num: 416 Reward: 2158.886124881817\n",
            "Total Timesteps: 388905 Episode Num: 417 Reward: 2204.427858694522\n",
            "Total Timesteps: 389905 Episode Num: 418 Reward: 2180.326496343539\n",
            "Total Timesteps: 390905 Episode Num: 419 Reward: 2199.326673584435\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2088.284163\n",
            "---------------------------------------\n",
            "Total Timesteps: 391905 Episode Num: 420 Reward: 1990.751957088351\n",
            "Total Timesteps: 392905 Episode Num: 421 Reward: 2185.75458718603\n",
            "Total Timesteps: 393905 Episode Num: 422 Reward: 2172.212776461285\n",
            "Total Timesteps: 394905 Episode Num: 423 Reward: 2208.7754963894504\n",
            "Total Timesteps: 395905 Episode Num: 424 Reward: 1970.937189818728\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2165.448273\n",
            "---------------------------------------\n",
            "Total Timesteps: 396905 Episode Num: 425 Reward: 2151.519681406337\n",
            "Total Timesteps: 397905 Episode Num: 426 Reward: 1997.9537422444514\n",
            "Total Timesteps: 398905 Episode Num: 427 Reward: 2079.0033187220447\n",
            "Total Timesteps: 399905 Episode Num: 428 Reward: 2148.281362562646\n",
            "Total Timesteps: 400905 Episode Num: 429 Reward: 2174.5237479336806\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2186.238618\n",
            "---------------------------------------\n",
            "Total Timesteps: 401905 Episode Num: 430 Reward: 2193.4798025231344\n",
            "Total Timesteps: 402905 Episode Num: 431 Reward: 2171.994252128462\n",
            "Total Timesteps: 403905 Episode Num: 432 Reward: 2136.8804655186495\n",
            "Total Timesteps: 404905 Episode Num: 433 Reward: 2102.169909959717\n",
            "Total Timesteps: 405905 Episode Num: 434 Reward: 2156.8913943341076\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2266.268830\n",
            "---------------------------------------\n",
            "Total Timesteps: 406905 Episode Num: 435 Reward: 2125.498245210687\n",
            "Total Timesteps: 407905 Episode Num: 436 Reward: 2191.9428936964878\n",
            "Total Timesteps: 408905 Episode Num: 437 Reward: 2210.985743634363\n",
            "Total Timesteps: 409905 Episode Num: 438 Reward: 2087.0938243946684\n",
            "Total Timesteps: 410905 Episode Num: 439 Reward: 2203.679079523972\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2184.344801\n",
            "---------------------------------------\n",
            "Total Timesteps: 411905 Episode Num: 440 Reward: 2213.3933720912505\n",
            "Total Timesteps: 412905 Episode Num: 441 Reward: 2219.818496777531\n",
            "Total Timesteps: 413905 Episode Num: 442 Reward: 2234.9328070562033\n",
            "Total Timesteps: 414905 Episode Num: 443 Reward: 2229.944016544979\n",
            "Total Timesteps: 415905 Episode Num: 444 Reward: 2292.9189304339893\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2201.657606\n",
            "---------------------------------------\n",
            "Total Timesteps: 416905 Episode Num: 445 Reward: 2165.9971791196917\n",
            "Total Timesteps: 417905 Episode Num: 446 Reward: 2216.783192160599\n",
            "Total Timesteps: 418905 Episode Num: 447 Reward: 2237.684079645907\n",
            "Total Timesteps: 419905 Episode Num: 448 Reward: 2121.488346070703\n",
            "Total Timesteps: 420905 Episode Num: 449 Reward: 2220.5817091024883\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2252.169143\n",
            "---------------------------------------\n",
            "Total Timesteps: 421905 Episode Num: 450 Reward: 2228.6388044490923\n",
            "Total Timesteps: 422905 Episode Num: 451 Reward: 2197.4810516550474\n",
            "Total Timesteps: 423905 Episode Num: 452 Reward: 2224.92020955223\n",
            "Total Timesteps: 424905 Episode Num: 453 Reward: 2166.9597735709544\n",
            "Total Timesteps: 425905 Episode Num: 454 Reward: 2264.992935945171\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2292.684341\n",
            "---------------------------------------\n",
            "Total Timesteps: 426905 Episode Num: 455 Reward: 2224.138016820048\n",
            "Total Timesteps: 427905 Episode Num: 456 Reward: 2254.308041415662\n",
            "Total Timesteps: 428905 Episode Num: 457 Reward: 2266.072249245233\n",
            "Total Timesteps: 429905 Episode Num: 458 Reward: 2167.8536927813066\n",
            "Total Timesteps: 430905 Episode Num: 459 Reward: 2160.3937995081674\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2284.512578\n",
            "---------------------------------------\n",
            "Total Timesteps: 431905 Episode Num: 460 Reward: 2257.9415181007857\n",
            "Total Timesteps: 432905 Episode Num: 461 Reward: 2209.35161258923\n",
            "Total Timesteps: 433905 Episode Num: 462 Reward: 2242.7797553178652\n",
            "Total Timesteps: 434905 Episode Num: 463 Reward: 2309.8362675038825\n",
            "Total Timesteps: 435905 Episode Num: 464 Reward: 2285.1272774980102\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2360.653355\n",
            "---------------------------------------\n",
            "Total Timesteps: 436905 Episode Num: 465 Reward: 2293.0827874437587\n",
            "Total Timesteps: 437905 Episode Num: 466 Reward: 2223.5696639885314\n",
            "Total Timesteps: 438905 Episode Num: 467 Reward: 2326.53837816921\n",
            "Total Timesteps: 439905 Episode Num: 468 Reward: 2213.182689292279\n",
            "Total Timesteps: 440905 Episode Num: 469 Reward: 2307.624625626164\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2394.046140\n",
            "---------------------------------------\n",
            "Total Timesteps: 441905 Episode Num: 470 Reward: 2290.6902916636227\n",
            "Total Timesteps: 442905 Episode Num: 471 Reward: 2087.39563539214\n",
            "Total Timesteps: 443905 Episode Num: 472 Reward: 2222.013858343797\n",
            "Total Timesteps: 444905 Episode Num: 473 Reward: 2181.9476371297\n",
            "Total Timesteps: 445905 Episode Num: 474 Reward: 2339.357133802227\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2412.591155\n",
            "---------------------------------------\n",
            "Total Timesteps: 446905 Episode Num: 475 Reward: 2374.971554366506\n",
            "Total Timesteps: 447905 Episode Num: 476 Reward: 2285.334618395444\n",
            "Total Timesteps: 448905 Episode Num: 477 Reward: 2257.8258422707504\n",
            "Total Timesteps: 449905 Episode Num: 478 Reward: 2185.435803810839\n",
            "Total Timesteps: 450905 Episode Num: 479 Reward: 2191.0833573825275\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2374.522087\n",
            "---------------------------------------\n",
            "Total Timesteps: 451905 Episode Num: 480 Reward: 2319.0742707832983\n",
            "Total Timesteps: 452905 Episode Num: 481 Reward: 2230.7010979572756\n",
            "Total Timesteps: 453905 Episode Num: 482 Reward: 2312.6976005978418\n",
            "Total Timesteps: 454905 Episode Num: 483 Reward: 2275.0101417314318\n",
            "Total Timesteps: 455905 Episode Num: 484 Reward: 2342.0059230847487\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2370.273638\n",
            "---------------------------------------\n",
            "Total Timesteps: 456905 Episode Num: 485 Reward: 2355.888389702039\n",
            "Total Timesteps: 457905 Episode Num: 486 Reward: 2311.8844823839054\n",
            "Total Timesteps: 458905 Episode Num: 487 Reward: 2363.592706285346\n",
            "Total Timesteps: 459905 Episode Num: 488 Reward: 2307.467506153942\n",
            "Total Timesteps: 460905 Episode Num: 489 Reward: 2317.8344839686933\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2297.763002\n",
            "---------------------------------------\n",
            "Total Timesteps: 461905 Episode Num: 490 Reward: 2262.4691439007547\n",
            "Total Timesteps: 462905 Episode Num: 491 Reward: 2215.2909267116484\n",
            "Total Timesteps: 463905 Episode Num: 492 Reward: 2066.782370067025\n",
            "Total Timesteps: 464905 Episode Num: 493 Reward: 2362.13672601849\n",
            "Total Timesteps: 465905 Episode Num: 494 Reward: 2334.047226201704\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2313.518343\n",
            "---------------------------------------\n",
            "Total Timesteps: 466905 Episode Num: 495 Reward: 2264.7235186162143\n",
            "Total Timesteps: 467905 Episode Num: 496 Reward: 2277.6999650888465\n",
            "Total Timesteps: 468905 Episode Num: 497 Reward: 2352.475213422005\n",
            "Total Timesteps: 469905 Episode Num: 498 Reward: 2351.103784642105\n",
            "Total Timesteps: 470905 Episode Num: 499 Reward: 2346.1769862302826\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2257.316580\n",
            "---------------------------------------\n",
            "Total Timesteps: 471905 Episode Num: 500 Reward: 2227.666095216993\n",
            "Total Timesteps: 472905 Episode Num: 501 Reward: 2327.6329052239453\n",
            "Total Timesteps: 473905 Episode Num: 502 Reward: 2280.053206918315\n",
            "Total Timesteps: 474905 Episode Num: 503 Reward: 2181.5026971701313\n",
            "Total Timesteps: 475905 Episode Num: 504 Reward: 2272.4857950710802\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2287.027400\n",
            "---------------------------------------\n",
            "Total Timesteps: 476905 Episode Num: 505 Reward: 2242.561164263603\n",
            "Total Timesteps: 477905 Episode Num: 506 Reward: 2405.0502298581846\n",
            "Total Timesteps: 478905 Episode Num: 507 Reward: 2365.3517131569542\n",
            "Total Timesteps: 479905 Episode Num: 508 Reward: 2444.861077548513\n",
            "Total Timesteps: 480905 Episode Num: 509 Reward: 2365.3392866520135\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2460.793784\n",
            "---------------------------------------\n",
            "Total Timesteps: 481905 Episode Num: 510 Reward: 2320.7566964572848\n",
            "Total Timesteps: 482905 Episode Num: 511 Reward: 2402.5044571269996\n",
            "Total Timesteps: 483905 Episode Num: 512 Reward: 2412.9387963334675\n",
            "Total Timesteps: 484905 Episode Num: 513 Reward: 2464.0904313068654\n",
            "Total Timesteps: 485905 Episode Num: 514 Reward: 2373.617071081826\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2443.291774\n",
            "---------------------------------------\n",
            "Total Timesteps: 486905 Episode Num: 515 Reward: 2299.175173173338\n",
            "Total Timesteps: 487905 Episode Num: 516 Reward: 2419.479981364704\n",
            "Total Timesteps: 488905 Episode Num: 517 Reward: 2336.223895467746\n",
            "Total Timesteps: 489905 Episode Num: 518 Reward: 2325.0428516557154\n",
            "Total Timesteps: 490905 Episode Num: 519 Reward: 2318.2254148683787\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2472.370464\n",
            "---------------------------------------\n",
            "Total Timesteps: 491905 Episode Num: 520 Reward: 2419.176146283512\n",
            "Total Timesteps: 492905 Episode Num: 521 Reward: 2403.3538976696213\n",
            "Total Timesteps: 493905 Episode Num: 522 Reward: 2431.9154404782525\n",
            "Total Timesteps: 494905 Episode Num: 523 Reward: 2423.939308386779\n",
            "Total Timesteps: 495905 Episode Num: 524 Reward: 2367.797642275355\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2464.602809\n",
            "---------------------------------------\n",
            "Total Timesteps: 496905 Episode Num: 525 Reward: 2453.9453843911783\n",
            "Total Timesteps: 497905 Episode Num: 526 Reward: 2406.7123331704397\n",
            "Total Timesteps: 498905 Episode Num: 527 Reward: 2445.2784936858498\n",
            "Total Timesteps: 499905 Episode Num: 528 Reward: 2352.334249732235\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2468.329495\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi6e2-_pu05e",
        "colab_type": "text"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW4d1YAMqif1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "82cd7973-a82c-48cc-f5cd-bfdb65ee08b2"
      },
      "source": [
        "class Actor(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x)) \n",
        "    return x\n",
        "\n",
        "class Critic(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    # Defining the second Critic neural network\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    # Forward-Propagation on the first Critic Neural Network\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Forward-Propagation on the second Critic Neural Network\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1\n",
        "\n",
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "\n",
        "class TD3(object):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \n",
        "    for it in range(iterations):\n",
        "      \n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state)\n",
        "      \n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "      \n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "      \n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "      \n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "      \n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "      \n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "      \n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      \n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "  \n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  \n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n",
        "\n",
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward\n",
        "\n",
        "env_name = \"AntBulletEnv-v0\"\n",
        "seed = 0\n",
        "\n",
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")\n",
        "\n",
        "eval_episodes = 20\n",
        "save_env_vid = True\n",
        "env = gym.make(env_name)\n",
        "max_episode_steps = env._max_episode_steps\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()\n",
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])\n",
        "policy = TD3(state_dim, action_dim, max_action)\n",
        "policy.load(file_name, './pytorch_models/')\n",
        "_ = evaluate_policy(policy, eval_episodes=eval_episodes)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_AntBulletEnv-v0_0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2487.773277\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}